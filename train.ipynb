{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RND = 777\n",
    "RUN = 'I.2'\n",
    "\n",
    "TEST_SIZE = .1\n",
    "\n",
    "N_BATCH_SAMPLES = 64\n",
    "N_EPOCHS = 1111\n",
    "\n",
    "MODELS_DIR = '/d3/caches/kaggle-integer-seq-v2/models/' + RUN\n",
    "TENSORBOARD_DIR = '/tmp-persistent/integer-seq-v2/' + RUN\n",
    "\n",
    "TRAIN_WITH_GENERATOR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODELS_DIR): os.makedirs(MODELS_DIR)\n",
    "if not os.path.isdir(TENSORBOARD_DIR): os.makedirs(TENSORBOARD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load stuff\n",
    "dictionary = np.load('out/dictionary@tr=10.npy').item()\n",
    "train_e_seq = np.load('out/train_e_seq@tr=10.npy').item()\n",
    "train_e_last = np.load('out/train_e_last@tr=10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DICT_SIZE = len(dictionary) + 1 # add 1 for zero-padding\n",
    "SEQ_LEN = len(train_e_seq.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train_e_seq.values(), dtype=np.int32)\n",
    "y_u = train_e_last.values() # unencoded\n",
    "\n",
    "X_train, X_val, y_u_train, y_u_val = \\\n",
    "    train_test_split(X, y_u, test_size=TEST_SIZE, random_state=RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per epoch: 91648 of 91665\n"
     ]
    }
   ],
   "source": [
    "# calculate samples per epoch\n",
    "n_sub_epochs = 1\n",
    "N_EPOCH_SAMPLES = (len(X_train) / n_sub_epochs / N_BATCH_SAMPLES) * N_BATCH_SAMPLES\n",
    "print 'Samples per epoch:', N_EPOCH_SAMPLES, 'of', len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation size: 10186\n",
      "Mem for val data: 0.52G\n"
     ]
    }
   ],
   "source": [
    "print 'Validation size:', len(y_u_val)\n",
    "print 'Mem for val data: %.2fG'%(DICT_SIZE * 4. * len(y_u_val) / 1024 / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate data batch\n",
    "def gen_batch(n_samples, batch_ix, X, y_u, dict_size=DICT_SIZE):\n",
    "    \n",
    "    batches_in_X = len(X) / n_samples\n",
    "    batch_ix %= batches_in_X\n",
    "\n",
    "    X_batch = X[batch_ix * n_samples:batch_ix * n_samples + n_samples]\n",
    "\n",
    "    y_batch = np.zeros([n_samples, dict_size], dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(y_batch)):\n",
    "        y_batch[i][y_u[i]] = 1.\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen validation data\n",
    "X_val, y_val = gen_batch(\n",
    "    n_samples=len(y_u_val), \n",
    "    batch_ix=0,\n",
    "    X=X_val,\n",
    "    y_u=y_u_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "lstm_output_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(DICT_SIZE, embedding_size, mask_zero=True, dropout=0.2))\n",
    "\n",
    "model.add(LSTM(input_dim=SEQ_LEN, output_dim=lstm_output_dim, return_sequences=False, dropout_U=0.2, dropout_W=0.2)) \n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(output_dim=DICT_SIZE, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, None, 64)      884672      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 128)           98816       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 13823)         1783167     dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2766655\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 91665\n",
      "Mem for tr data: 4.72G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91665 samples, validate on 10186 samples\n",
      "Epoch 1/1111\n",
      "32128/91665 [=========>....................] - ETA: 315s - loss: 6.6686 - acc: 0.0578"
     ]
    }
   ],
   "source": [
    "if TRAIN_WITH_GENERATOR:\n",
    "\n",
    "    batch_ix = -1\n",
    "\n",
    "    def gen_sample():\n",
    "\n",
    "        global batch_ix\n",
    "\n",
    "        while True:\n",
    "            batch_ix += 1\n",
    "            yield gen_batch(\n",
    "                n_samples=N_BATCH_SAMPLES,\n",
    "                batch_ix=batch_ix,\n",
    "                X=X_train,\n",
    "                y_u=y_u_train\n",
    "            )\n",
    "\n",
    "    history = model.fit_generator(\n",
    "            gen_sample(),\n",
    "            samples_per_epoch=N_EPOCH_SAMPLES,\n",
    "            nb_epoch=N_EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=True,\n",
    "            max_q_size=20,\n",
    "            nb_worker=1,\n",
    "            pickle_safe=False,\n",
    "            callbacks = [\n",
    "                keras.callbacks.TensorBoard(log_dir=TENSORBOARD_DIR, histogram_freq=0),\n",
    "                keras.callbacks.ModelCheckpoint(\n",
    "                    MODELS_DIR + \\\n",
    "                    '/e{epoch:02d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5', \n",
    "                    monitor='val_acc', verbose=0, save_best_only=False, \n",
    "                    save_weights_only=False, mode='auto'\n",
    "                ),\n",
    "            ]\n",
    "         )\n",
    "    \n",
    "else:\n",
    "\n",
    "    # gen train data\n",
    "    _X_train, _y_train = gen_batch(\n",
    "        n_samples=len(X_train), \n",
    "        batch_ix=0,\n",
    "        X=X_train,\n",
    "        y_u=y_u_train\n",
    "    )\n",
    "\n",
    "    print 'Training size:', len(y_u_train)\n",
    "    print 'Mem for tr data: %.2fG'%(DICT_SIZE * 4. * len(y_u_train) / 1024 / 1024 / 1024)\n",
    "\n",
    "    model.fit(\n",
    "        _X_train,\n",
    "        _y_train,\n",
    "        batch_size=N_BATCH_SAMPLES,\n",
    "        nb_epoch=N_EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks = [\n",
    "            keras.callbacks.TensorBoard(log_dir=TENSORBOARD_DIR, histogram_freq=0),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                MODELS_DIR + \\\n",
    "                '/e{epoch:02d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5', \n",
    "                monitor='val_acc', verbose=0, save_best_only=False, \n",
    "                save_weights_only=False, mode='auto'\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
